{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS DevDay Seoul 2019\n",
    "## 모두를 위한 컴퓨터 비전 딥러닝 툴킷, GluonCV 따라하기\n",
    "## Lab 2.2 Transfer Learning 적용하기\n",
    "\n",
    "<!-- This notebook is based on: https://github.com/zhreshold/ICCV19-GluonCV -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노트북을 처음 로딩할 때, Kernel로 **conda_mxnet_p36** 을 선택합니다.\n",
    "\n",
    "### 랩 순서\n",
    "\n",
    "1. Image Classification\n",
    "2. Object Detection - SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GluonCV와 필요한 python 패키지를 설치합니다.\n",
    "GluonCV의 model_zoo와 utils 패키지에 대해서는 아래 링크를 참조하세요.\n",
    "- model_zoo: [https://gluon-cv.mxnet.io/model_zoo/index.html](https://gluon-cv.mxnet.io/model_zoo/index.html)\n",
    "- utils: [https://gluon-cv.mxnet.io/api/utils.html](https://gluon-cv.mxnet.io/api/utils.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최초 실행시 GPU(p2/p3) instance에서는 아래 코드로 gluoncv 패키지를 설치하세요.\n",
    "!pip install --upgrade mxnet-cu100mkl gluoncv\n",
    "# CPU(c4/c5/m4/m5/t2/t3) instance에서는 아래 코드로 gluoncv 패키지를 설치하세요.\n",
    "#!pip install --upgrade mxnet-mkl gluoncv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, image, init, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "import gluoncv\n",
    "from gluoncv.utils import makedirs, download\n",
    "from gluoncv.model_zoo import get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여러분의 데이터셋을 사용해 Transfer Learning 적용하기\n",
    "\n",
    "잘 아시는 `ImageNet`은 백만개 이상의 이미지로 구성되어 있지만, 다른 도메인에서는 그처럼 방대한 labeled dataset을 구하기 쉽지 않습니다.\n",
    "Transfer learning은 바로 그럴 때 사용할 수 있는 테크닉으로, pretrained 모델의 학습 정보를 여러분의 도메인으로 옮겨오는 방법입니다.\n",
    "\n",
    "## 데이터 준비\n",
    "\n",
    "이번 랩에서는 `MINC-2500`이라는 데이터셋을 사용합니다. `MINC-2500`은 Cornell 대학교의 [MINC](http://opensurfaces.cs.cornell.edu/publications/minc/) 데이터셋의 부분집합으로, class당 2500개의 이미지, 총 23개의 class를 가지고 있습니다.\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/dmlc/web-data/master/gluoncv/datasets/MINC-2500.png \"MINC-2500 sample images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 실행하면, 실습을 위해 미리 준비된 `MINC-2500`의 일부를 다운로드 받아 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = 'https://raw.githubusercontent.com/dmlc/web-data/master/gluoncv/classification/minc-2500-tiny.zip'\n",
    "zip_file = download(file_url, path='./')\n",
    "with zipfile.ZipFile(zip_file, 'r') as zin:\n",
    "    zin.extractall(os.path.expanduser('./'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로, 각종 하이퍼파라미터를 설정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 23\n",
    "\n",
    "epochs = 5\n",
    "lr = 0.001\n",
    "per_device_batch_size = 1\n",
    "momentum = 0.9\n",
    "wd = 0.0001\n",
    "\n",
    "lr_factor = 0.75\n",
    "lr_steps = [10, 20, 30, np.inf]\n",
    "\n",
    "#주의: CPU instance를 사용한다면 num_gpus를 0으로 설정하세요.\n",
    "num_gpus = 1\n",
    "num_workers = 8\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n",
    "batch_size = per_device_batch_size * max(num_gpus, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to keep in mind:\n",
    "\n",
    "1. `epochs = 5` 부분은 이번 랩에서 작은 데이터셋을 사용하기 때문입니다. 큰 데이터셋을 사용한다면 이 값을 늘리세요(예: 40).\n",
    "\n",
    "2. `per_device_batch_size`도 역시 작게 지정되어 있습니다. 큰 데이터셋을 사용한다면 이 값도 늘려보시기 바랍니다(예: 64).\n",
    "\n",
    "3. `num_gpus`와 `num_workers` 값은 여러분이 사용하는 인스턴스의 종류에 따라 적절히 설정하세요.\n",
    "\n",
    "4. pretrained 모델이 이미 잘 만들어져 있으므로, 작은 `lr` 값으로 시작할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 transformation이라고도 하는 전처리 단계입니다. Transfer learning을 적용할 때 이 단계를 기본 모델과 동일하게 만드는 것이 중요합니다. \n",
    "\n",
    "아래 코드에서는 `ImageNet`과 동일한 전처리 과정을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_param = 0.4\n",
    "lighting_param = 0.1\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    transforms.RandomColorJitter(brightness=jitter_param, contrast=jitter_param,\n",
    "                                 saturation=jitter_param),\n",
    "    transforms.RandomLighting(lighting_param),\n",
    "    # from HWC to CHW\n",
    "    transforms.ToTensor(), \n",
    "    # set mean and std\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256, keep_ratio=True),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터를 `train`, `validation`, `test`로 로드합니다.\n",
    "\n",
    "`validation`과 `test`에도 동일한 전처리 과정을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로: 전체 데이터셋 또는 다른 데이터셋을 사용한다면 이 부분을 수정하세요.\n",
    "path = './minc-2500-tiny'\n",
    "\n",
    "train_path = os.path.join(path, 'train')\n",
    "val_path = os.path.join(path, 'val')\n",
    "test_path = os.path.join(path, 'test')\n",
    "\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.ImageFolderDataset(train_path).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.ImageFolderDataset(val_path).transform_first(transform_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers = num_workers)\n",
    "\n",
    "test_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.ImageFolderDataset(test_path).transform_first(transform_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터가 준비되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의와 학습\n",
    "\n",
    "이번 랩에서는 pretrained 모델로 `MobileNet1.0`을 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MobileNet1.0'\n",
    "# 다른 모델을 사용할 수도 있습니다.\n",
    "#model_name = 'ResNet50_v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning을 위해 새로운 모델(타겟 모델)을 아래와 같이 정의합니다.\n",
    "\n",
    "1. Pretrained 모델을 로드한다. (예: `ImageNet` 데이터셋으로 학습한 `MobileNet1.0`)\n",
    "2. 타겟 데이터셋(`MINC-2500`)의 클래스 수와 동일한 출력 크기를 가지는 새로운 output layer를 만들고, 이 layer의 파라미터를 랜덤 초기화한다.\n",
    "3. 타겟 데이터셋으로 위의 모델을 학습시킨다.\n",
    "\n",
    "이처럼 다른 task를 위해 학습된 모델을 새로운 dataset을 위해 tune하는 것을 **fine-tuning**이라고 부릅니다.\n",
    "\n",
    "![alt text](https://www.d2l.ai/_images/finetune.svg \"Fine tuning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_net = get_model(model_name, pretrained=True)\n",
    "with finetune_net.name_scope():\n",
    "    finetune_net.output = nn.Dense(classes)\n",
    "finetune_net.output.initialize(init.Xavier(), ctx = ctx)\n",
    "finetune_net.collect_params().reset_ctx(ctx)\n",
    "# convert execution style into symbolic programming\n",
    "finetune_net.hybridize()\n",
    "\n",
    "# Trainer, metric, loss\n",
    "trainer = gluon.Trainer(finetune_net.collect_params(), 'sgd', {\n",
    "                        'learning_rate': lr, 'momentum': momentum, 'wd': wd})\n",
    "metric = mx.metric.Accuracy()\n",
    "L = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로, `validation`과 `test` 데이터셋의 성능을 평가할 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, val_data, ctx):\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n",
    "        outputs = [net(X) for X in data]\n",
    "        metric.update(label, outputs)\n",
    "\n",
    "    return metric.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 루프\n",
    "\n",
    "이제 학습을 시작할 수 있습니다. \n",
    "\n",
    "`epochs` 하이퍼파라미터 값을 늘리거나, 더 큰 데이터셋을 사용하면 정확도를 더 높일 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_counter = 0\n",
    "num_batch = len(train_data)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if epoch == lr_steps[lr_counter]:\n",
    "        trainer.set_learning_rate(trainer.learning_rate*lr_factor)\n",
    "        lr_counter += 1\n",
    "\n",
    "    tic = time.time()\n",
    "    train_loss = 0\n",
    "    metric.reset()\n",
    "\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n",
    "        with ag.record():\n",
    "            outputs = [finetune_net(X) for X in data]\n",
    "            loss = [L(yhat, y) for yhat, y in zip(outputs, label)]\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        trainer.step(batch_size)\n",
    "        train_loss += sum([l.mean().asscalar() for l in loss]) / len(loss)\n",
    "\n",
    "        metric.update(label, outputs)\n",
    "\n",
    "    _, train_acc = metric.get()\n",
    "    train_loss /= num_batch\n",
    "\n",
    "    _, val_acc = test(finetune_net, val_data, ctx)\n",
    "\n",
    "    print('[Epoch %d] Train-acc: %.3f, loss: %.3f | Val-acc: %.3f | time: %.1f' %\n",
    "             (epoch, train_acc, train_loss, val_acc, time.time() - tic))\n",
    "\n",
    "_, test_acc = test(finetune_net, test_data, ctx)\n",
    "print('[Finished] Test-acc: %.3f' % (test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 간단한 예제로 image classification 모델의 transfer learning 과정을 배워봤습니다. 더 자세한 내용을 알고 싶으시면 `GluonCV`의 [관련 문서](https://gluon-cv.mxnet.io/build/examples_classification/transfer_learning_minc.html)를 방문해 보시기 바랍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Object Detection - SSD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 커스텀 데이터셋을 사용해 Transfer Learning 적용하기\n",
    "\n",
    "Object detection에도 transfer learning을 적용할 수 있습니다. \n",
    "Pikachu 데이터셋이라는 커스텀 데이터셋을 pretrained 모델에 학습시켜 피카추를 detection하는 예제를 실행해보겠습니다.\n",
    "\n",
    "## Pikachu 데이터셋\n",
    "\n",
    "Pikachu 데이터셋에 대해서는 이 [링크](https://www.d2l.ai/chapter_computer-vision/object-detection-dataset.html)를 참고하시기 바랍니다.\n",
    "\n",
    "![alt text](https://www.d2l.ai/_images/output_object-detection-dataset_42a473_7_0.png \"Pikachu dataset\")\n",
    "\n",
    "먼저 데이터셋을 다운로드합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, nd, image\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "import gluoncv\n",
    "from gluoncv.utils import download, viz\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import viz, download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/pikachu/train.rec'\n",
    "idx_url = 'https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/pikachu/train.idx'\n",
    "download(url, path='pikachu_train.rec', overwrite=False)\n",
    "download(idx_url, path='pikachu_train.idx', overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다운로드한 데이터셋을 로드해서 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = gluoncv.data.RecordFileDetection('pikachu_train.rec')\n",
    "classes = ['pikachu']  # only one foreground class here\n",
    "image, label = dataset[1]\n",
    "print('label:', label)\n",
    "# display image and label\n",
    "ax = viz.plot_bbox(image, bboxes=label[:, :4], labels=label[:, 4:5], class_names=classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 데이터셋은 오픈소스 3D 피카추 모델로부터 생성한 1000개의 다양한 피카추 이미지를 여러 풍경사진 배경 위에 합성한 것입니다.\n",
    "\n",
    "Pretrained `ssd_512_mobilenet1.0_voc` 모델을 로드해서 output layer를 리셋합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, 여러 object detection 모델 중에서 `SSD`를 사용해보겠습니다. 빠른 실행을 위해 `MobileNet1.0`을 기본으로 하는 `SSD`를 선택합니다.\n",
    "아래의 코드는 transfer learning을 위해 Pascal VOC로 pretrained 된 `ssd_512_mobilenet1.0_voc` 모델로부터 커스텀 네트웍을 만듭니다.\n",
    "\n",
    "`GluonCV Model Zoo`가 지원하는 detection 모델의 전체 목록은 [Model Zoo > Detection](https://gluon-cv.mxnet.io/model_zoo/detection.html) 링크에서 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = gluoncv.model_zoo.get_model('ssd_512_mobilenet1.0_custom', classes=classes, transfer='voc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning은 학습의 새로운 단계\n",
    "\n",
    "이제 dataloader를 아래 코드와 같이 정의합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(net, train_dataset, data_shape, batch_size, num_workers):\n",
    "    from gluoncv.data.batchify import Tuple, Stack, Pad\n",
    "    from gluoncv.data.transforms.presets.ssd import SSDDefaultTrainTransform\n",
    "    width, height = data_shape, data_shape\n",
    "    # use fake data to generate fixed anchors for target generation\n",
    "    with autograd.train_mode():\n",
    "        _, _, anchors = net(mx.nd.zeros((1, 3, height, width)))\n",
    "    batchify_fn = Tuple(Stack(), Stack(), Stack())  # stack image, cls_targets, box_targets\n",
    "    train_loader = gluon.data.DataLoader(\n",
    "        train_dataset.transform(SSDDefaultTrainTransform(width, height, anchors)),\n",
    "        batch_size, True, batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)\n",
    "    return train_loader\n",
    "\n",
    "train_data = get_dataloader(net, dataset, 512, 16, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습에 가능하면 GPU를 사용할 것을 권해드립니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n",
    "net.collect_params().reset_ctx(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 trainer, loss와 metric을 정의합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(\n",
    "    net.collect_params(), 'sgd',\n",
    "    {'learning_rate': 0.001, 'wd': 0.0005, 'momentum': 0.9})\n",
    "mbox_loss = gluoncv.loss.SSDMultiBoxLoss()\n",
    "ce_metric = mx.metric.Loss('CrossEntropy')\n",
    "smoothl1_metric = mx.metric.Loss('SmoothL1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 것이 준비되었으므로 이제 학습을 시작할 수 있습니다. 이 데이터셋에는 2번의 epoch면 충분합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, 2):\n",
    "    ce_metric.reset()\n",
    "    smoothl1_metric.reset()\n",
    "    tic = time.time()\n",
    "    btic = time.time()\n",
    "    net.hybridize(static_alloc=True, static_shape=True)\n",
    "    for i, batch in enumerate(train_data):\n",
    "        batch_size = batch[0].shape[0]\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        cls_targets = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        box_targets = gluon.utils.split_and_load(batch[2], ctx_list=ctx, batch_axis=0)\n",
    "        with autograd.record():\n",
    "            cls_preds = []\n",
    "            box_preds = []\n",
    "            for x in data:\n",
    "                cls_pred, box_pred, _ = net(x)\n",
    "                cls_preds.append(cls_pred)\n",
    "                box_preds.append(box_pred)\n",
    "            sum_loss, cls_loss, box_loss = mbox_loss(\n",
    "                cls_preds, box_preds, cls_targets, box_targets)\n",
    "            autograd.backward(sum_loss)\n",
    "        # since we have already normalized the loss, we don't want to normalize\n",
    "        # by batch-size anymore\n",
    "        trainer.step(1)\n",
    "        ce_metric.update(0, [l * batch_size for l in cls_loss])\n",
    "        smoothl1_metric.update(0, [l * batch_size for l in box_loss])\n",
    "        name1, loss1 = ce_metric.get()\n",
    "        name2, loss2 = smoothl1_metric.get()\n",
    "        if i % 1 == 0:\n",
    "            print('[Epoch {}][Batch {}], Speed: {:.3f} samples/sec, {}={:.3f}, {}={:.3f}'.format(\n",
    "                epoch, i, batch_size/(time.time()-btic), name1, loss1, name2, loss2))\n",
    "        btic = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning의 결과로 얻은 weight 값을 디스크에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_parameters('ssd_512_mobilenet1.0_pikachu.params')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning한 모델로 예측하기\n",
    "\n",
    "위에서 fine-tuning한 weight로 성능을 테스트할 수 있습니다. 여러 피카추가 들어있는 테스트 이미지로, 과연 모두 찾을 수 있을지 확인해 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = 'https://raw.githubusercontent.com/zackchase/mxnet-the-straight-dope/master/img/pikachu.jpg'\n",
    "download(test_url, 'pikachu_test.jpg')\n",
    "net = gluoncv.model_zoo.get_model('ssd_512_mobilenet1.0_custom', classes=classes, pretrained_base=False)\n",
    "net.load_parameters('ssd_512_mobilenet1.0_pikachu.params')\n",
    "x, image = gluoncv.data.transforms.presets.ssd.load_test('pikachu_test.jpg', 512)\n",
    "cid, score, bbox = net(x)\n",
    "ax = viz.plot_bbox(image, bbox[0], score[0], cid[0], class_names=classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5분도 걸리지 않은 두 번의 epoch만으로도 모든 피카추를 찾을 수 있었습니다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
