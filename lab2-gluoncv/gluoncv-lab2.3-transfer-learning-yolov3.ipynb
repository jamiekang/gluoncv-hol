{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS DevDay Seoul 2019\n",
    "## 모두를 위한 컴퓨터 비전 딥러닝 툴킷, GluonCV 따라하기\n",
    "## Lab 2.3 Transfer Learning 적용하기 - YOLOv3\n",
    "\n",
    "<!-- This notebook is based on: https://github.com/zhreshold/ICCV19-GluonCV -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노트북을 처음 로딩할 때, Kernel로 **conda_mxnet_p36** 을 선택합니다.\n",
    "\n",
    "### 랩 순서\n",
    "\n",
    "3. Object Detection - YOLOv3\n",
    "\n",
    "이번 랩에서는 앞 세션에 이어서 YOLOv3 object detection 모델에 transfer learning을 적용해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GluonCV와 필요한 python 패키지를 설치합니다.\n",
    "GluonCV의 model_zoo와 utils 패키지에 대해서는 아래 링크를 참조하세요.\n",
    "- model_zoo: [https://gluon-cv.mxnet.io/model_zoo/index.html](https://gluon-cv.mxnet.io/model_zoo/index.html)\n",
    "- utils: [https://gluon-cv.mxnet.io/api/utils.html](https://gluon-cv.mxnet.io/api/utils.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 최초 실행시 GPU(p2/p3) instance에서는 아래 코드로 gluoncv 패키지를 설치하세요.\n",
    "!pip install --upgrade mxnet-cu100mkl gluoncv\n",
    "# CPU(c4/c5/m4/m5/t2/t3) instance에서는 아래 코드로 gluoncv 패키지를 설치하세요.\n",
    "#!pip install --upgrade mxnet-mkl gluoncv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Object Detection - YOLOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비\n",
    "\n",
    "아래 코드를 실행하면, 실습을 위해 미리 준비된 데이터셋에 접근할 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, nd, image\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "import gluoncv\n",
    "from gluoncv.utils import download, viz\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import viz, download\n",
    "from gluoncv.data.batchify import Tuple, Stack, Pad\n",
    "from gluoncv.data.transforms.presets.yolo import YOLO3DefaultTrainTransform\n",
    "from gluoncv.data.transforms.presets.yolo import YOLO3DefaultValTransform\n",
    "from gluoncv.data.dataloader import RandomTransformDataLoader\n",
    "from gluoncv.utils.metrics.voc_detection import VOC07MApMetric, VOCMApMetric\n",
    "from gluoncv.utils import LRScheduler, LRSequential\n",
    "\n",
    "#주의: CPU instance를 사용한다면 num_gpus를 0으로 설정하세요.\n",
    "num_gpus = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다운로드한 데이터셋을 로드해서 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = gluoncv.data.RecordFileDetection('train.rec')\n",
    "val_dataset = gluoncv.data.RecordFileDetection('val.rec')\n",
    "classes = ['bird', 'boat', 'car', 'person']\n",
    "image, label = train_dataset[10]\n",
    "print('label:', label)\n",
    "# display image and label\n",
    "ax = viz.plot_bbox(image, bboxes=label[:, :4], labels=label[:, 4:5], class_names=classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, 여러 object detection 모델 중에서 `YOLO`를 사용해보겠습니다. 빠른 실행을 위해 `darknet53`을 기본으로 하는 `YOLOv3`를 선택합니다.\n",
    "아래의 코드는 transfer learning을 위해 Pascal VOC로 pretrained 된 `yolo3_darknet53_voc` 모델로부터 커스텀 네트웍을 만듭니다.\n",
    "\n",
    "`GluonCV Model Zoo`가 지원하는 detection 모델의 전체 목록은 [Model Zoo > Detection](https://gluon-cv.mxnet.io/model_zoo/detection.html) 링크에서 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluoncv.model_zoo.get_model('yolo3_darknet53_custom', classes=classes, transfer='voc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning은 학습의 새로운 단계\n",
    "\n",
    "이제 dataloader를 아래 코드와 같이 정의합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape = 416\n",
    "batch_size = 12\n",
    "num_workers = 2\n",
    "\n",
    "def get_dataloader(net, train_dataset, val_dataset, data_shape, batch_size, num_workers):\n",
    "    \"\"\"Get dataloader.\"\"\"\n",
    "    width, height = data_shape, data_shape\n",
    "\n",
    "    batchify_fn = Tuple(*([Stack() for _ in range(6)] + [Pad(axis=0, pad_val=-1) for _ in range(1)]))  # stack image, all targets generated\n",
    "\n",
    "    train_loader = gluon.data.DataLoader(\n",
    "        train_dataset.transform(YOLO3DefaultTrainTransform(width, height, net, mixup=None)),\n",
    "        batch_size, True, batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)\n",
    "    \n",
    "    val_batchify_fn = Tuple(Stack(), Pad(pad_val=-1))\n",
    "    \n",
    "    val_loader = gluon.data.DataLoader(\n",
    "        val_dataset.transform(YOLO3DefaultValTransform(width, height)),\n",
    "        batch_size, False, batchify_fn=val_batchify_fn, last_batch='keep', num_workers=num_workers)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_data, val_data = get_dataloader(net, train_dataset, val_dataset, data_shape, batch_size, num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습에 가능하면 GPU를 사용할 것을 권해드립니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n",
    "net.collect_params().reset_ctx(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 trainer, loss와 metric을 정의합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(\n",
    "    net.collect_params(), 'sgd',\n",
    "    {'learning_rate': 0.001, 'wd': 0.0005, 'momentum': 0.9})\n",
    "\n",
    "# targets\n",
    "sigmoid_ce = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "l1_loss = gluon.loss.L1Loss()\n",
    "\n",
    "# metrics\n",
    "obj_metrics = mx.metric.Loss('ObjLoss')\n",
    "center_metrics = mx.metric.Loss('BoxCenterLoss')\n",
    "scale_metrics = mx.metric.Loss('BoxScaleLoss')\n",
    "cls_metrics = mx.metric.Loss('ClassLoss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 것이 준비되었으므로 이제 학습을 시작할 수 있습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "best_map =[0]\n",
    "\n",
    "obj_loss_list = []\n",
    "boxcenter_loss_list = []\n",
    "boxscale_loss_list = []\n",
    "classloss_list = []\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    tic = time.time()\n",
    "    btic = time.time()\n",
    "    mx.nd.waitall()\n",
    "    net.hybridize(static_alloc=True, static_shape=True)\n",
    "            \n",
    "    for i, batch in enumerate(train_data):\n",
    "        batch_size = batch[0].shape[0]\n",
    "        \n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        # objectness, center_targets, scale_targets, weights, class_targets\n",
    "        fixed_targets = [gluon.utils.split_and_load(batch[it], ctx_list=ctx, batch_axis=0) for it in range(1, 6)]\n",
    "        gt_boxes = gluon.utils.split_and_load(batch[6], ctx_list=ctx, batch_axis=0)\n",
    "        \n",
    "        sum_losses = []\n",
    "        obj_losses = []\n",
    "        center_losses = []\n",
    "        scale_losses = []\n",
    "        cls_losses = []\n",
    "        \n",
    "        with autograd.record():\n",
    "            for ix, x in enumerate(data):\n",
    "                obj_loss, center_loss, scale_loss, cls_loss = net(x, gt_boxes[ix], *[ft[ix] for ft in fixed_targets])\n",
    "                sum_losses.append(obj_loss + center_loss + scale_loss + cls_loss)\n",
    "                obj_losses.append(obj_loss)\n",
    "                center_losses.append(center_loss)\n",
    "                scale_losses.append(scale_loss)\n",
    "                cls_losses.append(cls_loss)\n",
    "            autograd.backward(sum_losses)\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        obj_metrics.update(0, obj_losses)\n",
    "        center_metrics.update(0, center_losses)\n",
    "        scale_metrics.update(0, scale_losses)\n",
    "        cls_metrics.update(0, cls_losses)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            name1, loss1 = obj_metrics.get()\n",
    "            name2, loss2 = center_metrics.get()\n",
    "            name3, loss3 = scale_metrics.get()\n",
    "            name4, loss4 = cls_metrics.get()                \n",
    "            print('[Epoch {}][Batch {}], LR: {:.2E}, Speed: {:.3f} samples/sec, {}={:.3f}, {}={:.3f}, {}={:.3f}, {}={:.3f}'.format(\n",
    "                epoch, i, trainer.learning_rate, batch_size/(time.time()-btic), name1, loss1, name2, loss2, name3, loss3, name4, loss4))\n",
    "        btic = time.time()\n",
    "\n",
    "    name1, loss1 = obj_metrics.get()\n",
    "    name2, loss2 = center_metrics.get()\n",
    "    name3, loss3 = scale_metrics.get()\n",
    "    name4, loss4 = cls_metrics.get()\n",
    "    \n",
    "    obj_loss_list.append(loss1)\n",
    "    boxcenter_loss_list.append(loss2)\n",
    "    boxscale_loss_list.append(loss3)\n",
    "    classloss_list.append(loss4)\n",
    "    \n",
    "    print('[Epoch {}] Training cost: {:.3f}, {}={:.3f}, {}={:.3f}, {}={:.3f}, {}={:.3f}'.format(\n",
    "        epoch, (time.time()-tic), name1, loss1, name2, loss2, name3, loss3, name4, loss4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(obj_loss_list, label='object loss')\n",
    "plt.plot(classloss_list, label='class loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning의 결과로 얻은 weight 값을 디스크에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_parameters('yolo3_darknet53_voc_0000.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning한 모델로 예측하기\n",
    "\n",
    "위에서 fine-tuning한 weight로 성능을 테스트할 수 있습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "net_name = 'yolo3_darknet53_custom'\n",
    "model_param_fname = 'yolo3_darknet53_voc_0000.params'\n",
    "# model_param_fname = 'yolo3_darknet53_voc_best.params'\n",
    "classes = ['bird', 'boat', 'car', 'person']\n",
    "\n",
    "# ctx = mx.cpu()\n",
    "ctx = mx.gpu(0)\n",
    "    \n",
    "net = gluoncv.model_zoo.get_model(net_name, classes=classes, ctx=ctx, pretrained_base=False)\n",
    "net.load_parameters(model_param_fname, ctx=ctx)\n",
    "net.hybridize()\n",
    "\n",
    "x, image = gluoncv.data.transforms.presets.yolo.load_test('test_sample_0.jpg', 416)\n",
    "x = x.as_in_context(ctx)\n",
    "print('Shape of pre-processed image:', x.shape)\n",
    "cid, score, bbox = net(x)\n",
    "\n",
    "ax = viz.plot_bbox(image, bbox[0], score[0], cid[0], class_names=classes)\n",
    "plt.figure(figsize=(32,32))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 `GluonCV`에서 `YOLOv3` 기반의 커스텀 네트워크를 사용해 object detection하는 예제를 알아봤습니다. 수고하셨습니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
